@misc{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeffrey Wu and et al},
  year={2019}
}

@misc{bpe,
    title = {Neural Machine Translation of Rare Words with Subword Units},
    author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
    year = {2016},
}

@misc{transformer,
  title={Attention Is All You Need},
  author={Ashish Vaswani and et al.},
  year={2017}
}

@misc{xception,
  author = {Fran√ßois Chollet},
  title = {Xception: Deep Learning with Depthwise Separable Convolutions},
  year = {2016}
}

@misc{gelu,
  title={Gaussian Error Linear Units (GELUs)},
  author={Dan Hendrycks and Kevin Gimpel},
  year={2016}
}

@misc{glue,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  year={2019}
}

@misc{glue_human,
  title={A Conservative Human Baseline Estimate for GLUE:People Still (Mostly) Beat Machines},
  author={Nangia Nikita and Bowman Samuel R.},
  year={2019}
}

@misc{bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and et al.},
  year={2018}
}

@misc{evolved,
  author = {So, David and Liang, Chen and Le, Quoc},
  year = {2019},
  title = {The Evolved Transformer}
}

@misc{transformerxl,
  author={Zihang Dai and Zhilin Yang and et al.},
  year = {2019},
  title={Transformer-{XL}: Language Modeling with Longer-Term Dependency}
}

@misc{megatronlm,
  title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and et al.},
  year = {2019}
}

@misc{xlnet,
  author = {Yang, Zhilin and Dai, Zihang and et al.},
  title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  year = 2019
}

@misc{roberta,
  author = {Yinhan Liu and et al.},
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  year = 2019
}

@misc{structbert,
  title = {StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding},
  author = {Wei Wang and et al.},
  year = {2019}
}

@misc{xlm,
  title = {Cross-lingual Language Model Pretraining},
  author = {Guillaume Lample, Alexis Conneau},
  year = {2019},
}

@misc{tidybert,
  title = {TinyBERT: Distilling BERT for Natural Language Understanding},
  author = {Xiaoqi Jiao and et al.},
  year = {2019}
}

@misc{ctrl,
  title = {CTRL - A Conditional Transformer Language Model for Controllable Generation},
  author = {Keskar, Nitish Shirish and et al.},
  year = {2019}
}

@misc{albert,
  title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author = {Zhenzhong Lan and et al.},
  year = {2019}
}

@misc{t5,
  author = {Colin Raffel and et al.},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  year = {2019}
}

@misc{sparse_transformer,
  title={Generating Long Sequences with Sparse Transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  year={2019}
}

@misc{longformer,
  title={Longformer: The Long-Document Transformer},
  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
  year={2020}
}

@misc{synthesizer,
  author = {Tay, Yi and Bahri, Dara and Metzler, Donald and Juan, Da-Cheng and Zhao, Zhe and Zheng, Che},
  year = {2020},
  title = {Synthesizer: Rethinking Self-Attention in Transformer Models}
}

@misc{linformer,
    title={Linformer: Self-Attention with Linear Complexity},
    author={Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
    year={2020}
}

@misc{sandwich_transformers,
  author = {Ofir Press, Noah A. Smith, Omer Levy},
  year = {2020},
  title = {Improving Transformer Models by Reordering their Sublayers}
}

@misc{mpnet,
    title={MPNet: Masked and Permuted Pre-training for Language Understanding},
    author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
    year={2020}
}

@misc{igpt,
  title={Generative Pretraining from Pixels},
  author={Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeff and Jun, Heewoo and Dhariwal, Prafulla and Luan, David and Sutskever, Ilya},
  year={2020}
}

@misc{reformer,
  title={Reformer: The Efficient Transformer},
  author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
  year={2020}
}

@misc{adaptively_sparse_transformers,
  author={G. Correia and V. Niculae and A. Martins},
  title={Adaptively Sparse Transformers},
  year={2019}
}

@misc{adaptive_span,
  title = {Adaptive Attention Span in Transformers},
  author = {Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
  year = {2019}
}

@misc{gshard,
  title = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  author = {Lepikhin, Dmitry and Lee, HyoukJoong and  Xu, Yuanzhong},
  year = {2020}
}

@misc{n_brc,
  title = {A bio-inspired bistable recurrent cell allows for long-lasting memory},
  author = {Vecoven, Nicolas and Ernst, Damien and Drion, Guillaume},
  year = {2020}
}

@misc{distil_bert,
  title = {DistilBERT, a distilled version of BERT: smaller,faster, cheaper and lighter},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wold, Thomas},
  year = {2019}
}

@misc{tiny_bert,
  title = {TinyBERT: Distilling BERT for Natural Language Understanding},
  author = {Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  year = {2019}
}

@misc{mobile_bert,
  title = {MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices},
  author = {Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
  year = {2020}
}

@misc{mini_lm,
  title = {MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers},
  author = {Wan, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  year = {2020}
}

@misc{shortformer,
  title = {Shortformer: Better Language Modeling using Shorter Inputs},
  author = {Press, Ofir and Smith A., Noah and Lewis, Mike},
  year = {2020}
}

@misc{switch_transformer,
  title = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  year = {2021}
}

@misc{feedback_transformer,
  title = {Addressing Some Limitations of Transformers with Feedback Memory},
  author = {Fan, Angela and Lavril, Thibaut and Grave, Edouard and Joulin, Armand and Sukhbaatar, Sainbayar},
  year = {2020}
}

@misc{moe_layer,
  title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  year = {2017}
}

@misc{l2l,
  title = {Training Large Neural Networks with Constant Memory using a New Execution Algorithm},
  author = {Pudipeddi, Bharadwaj and Mesmakhosroshahi Maral and Xi, Jinwen and Bharadwaj, Sujeeth},
  year = {2020}
}

@misc{gpipe,
  title = {GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
  author = {Huang, Yanping and Yonglong, Cheng and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc and Chen, Zhifeng},
  year = {2020}
}

@misc{pipedream,
  title = {PipeDream: Generalized Pipeline Parallelism for DNN Training},
  author = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil and Granger, Greg and Gibbons, Phil and Zaharia, Matei},
  year = {2019},
}

@misc{mesh_tensorflow,
  title = {Mesh-TensorFlow: deep learning for supercomputers},
  author = {Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and Sepassi, Ryan and Hechtman, Blake},
  year = {2018}
}

@misc{zero,
  title = {ZeRO: Memory Optimizations Toward Training Trillion Parameter Models},
  author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  year = {2019}
}

@misc{zero_offload,
  title = {ZeRO-Offload: Democratizing Billion-Scale Model Training},
  author = {Ren, Jie and Rajbhandari, Samyam and Aminabadi, Yazdani, Reza and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  year = {2021}
}

@misc{zero_infinity,
  title = {ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning},
  author = {Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
  year = {2021}
}

@misc{megatronlm_2,
  title = {Efficient Large-Scale Language Model Training on GPU Clusters},
  author = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and et al.},
  year = {2021}
}

@misc{gpt3,
  title = {Language Models are Few-Shot Learners},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and et al.},
  year = {2020}
}

@misc{flan,
  title = {Finetuned Language Models Are Zero-Shot Learners},
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and et al.},
  year = {2021}
}

@misc{prefix_tuning,
  title = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author = {Lisa Li, Xiang and Liang, Percy},
  year = {2021}
}

@misc{scale_prefix_tuning,
  title = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  year = {2021}
}
