@misc{gpt2,
    title = {Language Models are Unsupervised Multitask Learners},
    author = {Alec Radford and Jeffrey Wu and et al},
    year = {2019}
}

@misc{bpe,
    title = {Neural Machine Translation of Rare Words with Subword Units},
    author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
    year = {2016},
}

@misc{transformer,
    title = {Attention Is All You Need},
    author = {Ashish Vaswani and et al.},
    year = {2017}
}

@misc{xception,
    author = {François Chollet},
    title = {Xception: Deep Learning with Depthwise Separable Convolutions},
    year = {2016}
}

@misc{gelu,
    title = {Gaussian Error Linear Units (GELUs)},
    author = {Dan Hendrycks and Kevin Gimpel},
    year = {2016}
}

@misc{glue,
    title = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
    author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
    year = {2019}
}

@misc{glue_human,
    title = {A Conservative Human Baseline Estimate for GLUE:People Still (Mostly) Beat Machines},
    author = {Nangia Nikita and Bowman Samuel R.},
    year = {2019}
}

@misc{bert,
    title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author = {Devlin, Jacob and et al.},
    year = {2018}
}

@misc{evolved,
    author = {So, David and Liang, Chen and Le, Quoc},
    year = {2019},
    title = {The Evolved Transformer}
}

@misc{transformerxl,
    author = {Zihang Dai and Zhilin Yang and et al.},
    year = {2019},
    title = {Transformer-{XL}: Language Modeling with Longer-Term Dependency}
}

@misc{megatronlm,
    title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
    author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and et al.},
    year = {2019}
}

@misc{xlnet,
    author = {Yang, Zhilin and Dai, Zihang and et al.},
    title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
    year = 2019
}

@misc{roberta,
    author = {Yinhan Liu and et al.},
    title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    year = 2019
}

@misc{structbert,
    title = {StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding},
    author = {Wei Wang and et al.},
    year = {2019}
}

@misc{xlm,
    title = {Cross-lingual Language Model Pretraining},
    author = {Guillaume Lample, Alexis Conneau},
    year = {2019},
}

@misc{tidybert,
    title = {TinyBERT: Distilling BERT for Natural Language Understanding},
    author = {Xiaoqi Jiao and et al.},
    year = {2019}
}

@misc{ctrl,
    title = {CTRL - A Conditional Transformer Language Model for Controllable Generation},
    author = {Keskar, Nitish Shirish and et al.},
    year = {2019}
}

@misc{albert,
    title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
    author = {Zhenzhong Lan and et al.},
    year = {2019}
}

@misc{t5,
    author = {Colin Raffel and et al.},
    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
    year = {2019}
}

@misc{sparse_transformer,
    title = {Generating Long Sequences with Sparse Transformers},
    author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
    year = {2019}
}

@misc{longformer,
    title = {Longformer: The Long-Document Transformer},
    author = {Iz Beltagy and Matthew E. Peters and Arman Cohan},
    year = {2020}
}

@misc{synthesizer,
    author = {Tay, Yi and Bahri, Dara and Metzler, Donald and Juan, Da-Cheng and Zhao, Zhe and Zheng, Che},
    year = {2020},
    title = {Synthesizer: Rethinking Self-Attention in Transformer Models}
}

@misc{linformer,
    title = {Linformer: Self-Attention with Linear Complexity},
    author = {Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
    year = {2020}
}

@misc{sandwich_transformers,
    author = {Ofir Press, Noah A. Smith, Omer Levy},
    year = {2020},
    title = {Improving Transformer Models by Reordering their Sublayers}
}

@misc{mpnet,
    title = {MPNet: Masked and Permuted Pre-training for Language Understanding},
    author = {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
    year = {2020}
}

@misc{igpt,
    title = {Generative Pretraining from Pixels},
    author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeff and Jun, Heewoo and Dhariwal, Prafulla and Luan, David and Sutskever, Ilya},
    year = {2020}
}

@misc{reformer,
    title = {Reformer: The Efficient Transformer},
    author = {Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
    year = {2020}
}

@misc{adaptively_sparse_transformers,
    author = {G. Correia and V. Niculae and A. Martins},
    title = {Adaptively Sparse Transformers},
    year = {2019}
}

@misc{adaptive_span,
    title = {Adaptive Attention Span in Transformers},
    author = {Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
    year = {2019}
}

@misc{gshard,
    title = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
    author = {Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong},
    year = {2020}
}

@misc{n_brc,
    title = {A bio-inspired bistable recurrent cell allows for long-lasting memory},
    author = {Vecoven, Nicolas and Ernst, Damien and Drion, Guillaume},
    year = {2020}
}

@misc{distil_bert,
    title = {DistilBERT, a distilled version of BERT: smaller,faster, cheaper and lighter},
    author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wold, Thomas},
    year = {2019}
}

@misc{tiny_bert,
    title = {TinyBERT: Distilling BERT for Natural Language Understanding},
    author = {Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
    year = {2019}
}

@misc{mobile_bert,
    title = {MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices},
    author = {Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
    year = {2020}
}

@misc{mini_lm,
    title = {MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers},
    author = {Wan, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
    year = {2020}
}

@misc{shortformer,
    title = {Shortformer: Better Language Modeling using Shorter Inputs},
    author = {Press, Ofir and Smith A., Noah and Lewis, Mike},
    year = {2020}
}

@misc{switch_transformer,
    title = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
    author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
    year = {2021}
}

@misc{feedback_transformer,
    title = {Addressing Some Limitations of Transformers with Feedback Memory},
    author = {Fan, Angela and Lavril, Thibaut and Grave, Edouard and Joulin, Armand and Sukhbaatar, Sainbayar},
    year = {2020}
}

@misc{moe_layer,
    title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
    author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
    year = {2017}
}

@misc{l2l,
    title = {Training Large Neural Networks with Constant Memory using a New Execution Algorithm},
    author = {Pudipeddi, Bharadwaj and Mesmakhosroshahi Maral and Xi, Jinwen and Bharadwaj, Sujeeth},
    year = {2020}
}

@misc{gpipe,
    title = {GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
    author = {Huang, Yanping and Yonglong, Cheng and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc and Chen, Zhifeng},
    year = {2020}
}

@misc{pipedream,
    title = {PipeDream: Generalized Pipeline Parallelism for DNN Training},
    author = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil and Granger, Greg and Gibbons, Phil and Zaharia, Matei},
    year = {2019},
}

@misc{mesh_tensorflow,
    title = {Mesh-TensorFlow: deep learning for supercomputers},
    author = {Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and Sepassi, Ryan and Hechtman, Blake},
    year = {2018}
}

@misc{zero,
    title = {ZeRO: Memory Optimizations Toward Training Trillion Parameter Models},
    author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
    year = {2019}
}

@misc{zero_offload,
    title = {ZeRO-Offload: Democratizing Billion-Scale Model Training},
    author = {Ren, Jie and Rajbhandari, Samyam and Aminabadi, Yazdani, Reza and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
    year = {2021}
}

@misc{zero_infinity,
    title = {ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning},
    author = {Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
    year = {2021}
}

@misc{megatronlm_2,
    title = {Efficient Large-Scale Language Model Training on GPU Clusters},
    author = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and et al.},
    year = {2021}
}

@misc{gpt3,
    title = {Language Models are Few-Shot Learners},
    author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and et al.},
    year = {2020}
}

@misc{flan,
    title = {Finetuned Language Models Are Zero-Shot Learners},
    author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and et al.},
    year = {2021}
}

@misc{prefix_tuning,
    title = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
    author = {Lisa Li, Xiang and Liang, Percy},
    year = {2021}
}

@misc{scale_prefix_tuning,
    title = {The Power of Scale for Parameter-Efficient Prompt Tuning},
    author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
    year = {2021}
}

@misc{deepspeed_megatron,
    title = {Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model},
    author = {Smith, Shaden and Patwary, Mostofa and et al.},
    year = {2022}
}

@misc{gopher,
    title = {Scaling Language Models: Methods, Analysis \& Insights from Training Gopher},
    author = {Rae, W. Jack and Borgeaud, Sebastian and Cai, Trevor and et al.},
    year = {2021}
}

@misc{blog_turing_nlg,
    title = {Turing-NLG: A 17-billion-parameter language model by Microsoft},
    year = {February 13, 2020 (accessed March 29, 2021)},
    howpublished = "\url{https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/}"
}

@misc{blog_mt_nlg,
    title = {Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model},
    year = {October 11, 2021 (accessed March 29, 2021)},
    howpublished = "\url{https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/}"
}

@misc{blog_gpt_neox_20b,
    title = {Announcing GPT-NeoX-20B},
    year = {February 2, 2022 (accessed March 29, 2021)},
    howpublished = "\url{https://blog.eleuther.ai/announcing-20b/}"
}

@misc{blog_torch_fsdp,
    title = {Introducing PyTorch Fully Sharded Data Parallel (FSDP) API},
    year = {March 14, 2022 (accessed March 29, 2021)},
    howpublished = "\url{https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/}"
}

@misc{jax,
    author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake VanderPlas and Skye Wanderman-Milne and Qiao Zhang},
    title = {JAX: composable transformations of Python+NumPy programs},
    url = {http://github.com/google/jax},
    year = {2018},
}

@misc{jax_haiku,
    author = {Tom Hennigan and Trevor Cai and Tamara Norman and Igor Babuschkin},
    title = {Haiku: Sonnet for JAX},
    url = {http://github.com/deepmind/dm-haiku},
    year = {2020},
}

@misc{gpt_neox,
    title = {{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},
    author = {Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
    year = {2022}
}

@misc{roformer,
    title = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
    author = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},
    year = {2021}
}

@misc{xglm,
    author = {Xi Victoria Lin and Xian Li and et al.},
    title = {Few-shot Learning with Multilingual Language Models},
    year = {2021}
}

@misc{lamda,
    title = {LaMDA: Language Models for Dialog Applications},
    author = {Aaron Daniel Cohen and Adam Roberts and Alejandra Molina and Alena Butryna and Alicia Jin and Apoorv Kulshreshtha and Ben Hutchinson and Ben Zevenbergen and Blaise Hilary Aguera-Arcas and Chung-ching Chang and Claire Cui and Cosmo Du and Daniel De Freitas Adiwardana and Dehao Chen and Dmitry (Dima) Lepikhin and Ed H. Chi and Erin Hoffman-John and Heng-Tze Cheng and Hongrae Lee and Igor Krivokon and James Qin and Jamie Hall and Joe Fenton and Johnny Soraker and Kathy Meier-Hellstern and Kristen Olson and Lora Mois Aroyo and Maarten Paul Bosma and Marc Joseph Pickett and Marcelo Amorim Menegali and Marian Croak and Mark Díaz and Matthew Lamm and Maxim Krikun and Meredith Ringel Morris and Noam Shazeer and Quoc V. Le and Rachel Bernstein and Ravi Rajakumar and Ray Kurzweil and Romal Thoppilan and Steven Zheng and Taylor Bos and Toju Duke and Tulsee Doshi and Vinodkumar Prabhakaran and Will Rusch and YaGuang Li and Yanping Huang and Yanqi Zhou and Yuanzhong Xu and Zhifeng Chen},
    year = {2022}
}

@misc{pangu,
    title = {PanGu-$\alpha$: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation},
    author = {Wei Zeng and Xiaozhe Ren and Teng Su and Hui Wang and Yi Liao and Zhiwei Wang and Xin Jiang and ZhenZhang Yang and Kaisheng M. Wang and Xiaoda Zhang and Chen Li and Ziyan Gong and Yifan Yao and Xinjing Huang and Jun Wang and Jianfeng Yu and Qiwei Guo and Yue Yu and Yan Zhang and Jin Wang and Heng Tao and Dasen Yan and Zexuan Yi and Fang Peng and Fan Jiang and Han Zhang and Lingfeng Deng and Yehong Zhang and Zhengping Lin and Chao Zhang and Shaojie Zhang and Mingyue Guo and Shanzhi Gu and Gaojun Fan and Yaowei Wang and Xuefeng Jin and Qun Liu and Yonghong Tian},
    year = {2021},
}

@misc{deepnet,
    title = {DeepNet: Scaling Transformers to 1, 000 Layers},
    author = {Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Dongdong Zhang and Furu Wei},
    year = {2022}
}

@misc{polycoder,
    title = {A Systematic Evaluation of Large Language Models of Code},
    author = {Xu, Frank F and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent J},
    year = {2022}
}
