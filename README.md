# Presentations

- [2018_08_28](presentation_2018_08_28)
  - Unsupervised learning methods based on huge data for the needs of supervised learning methods using small training sets
  - Publications:
    - CoVe - [Learned in Translation: Contextualized Word Vectors](https://arxiv.org/abs/1708.00107)
    - ELMo - [Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples](https://arxiv.org/abs/1805.06556)
- [2018_11_04](presentation_2018_11_04)
  - Focused Hierarchical RNNs
  - Publications:
    - Focused Hierarchical RNN - [Focused Hierarchical RNNs for Conditional Sequence Processing](https://arxiv.org/abs/1806.04342)
- [2019_03_12](presentation_2019_03_12)
  - History of the forbidden model
  - Publications:
    - Transformer - [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
    - GLUE Benchmark - [GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://arxiv.org/abs/1804.07461)
    - BERT - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
    - GPT-2 - [Language Models are Unsupervised Multitask Learners](https://openai.com/blog/better-language-models/)
- [2019_11_04](presentation_2019_11_04)
  - History of the forbidden model
  - Publications:
    - Transformer - [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
    - GPT-2 - [Language Models are Unsupervised Multitask Learners](https://openai.com/blog/better-language-models/)
    - BERT - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
    - Evolved Transformer - [The Evolved Transformer](https://arxiv.org/abs/1901.11117)
    - XLM - [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291)
    - TransformerXL - [Transformer-XL: Language Modeling with Longer-Term Dependency](https://openreview.net/forum?id=HJePno0cYm)
    - Megatron-LM - [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
    - XLNet - [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)
    - RoBERTa - [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
    - StructBERT - [StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding](https://arxiv.org/abs/1908.04577)
    - ALBERT - [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
    - TinyBERT - [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)
    - CTRL - [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858)
    - T5 - [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
    - GLUE Benchmark - [GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://arxiv.org/abs/1804.07461)
    - SuperGLUE Benchmark - [SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems](https://arxiv.org/abs/1905.00537)
- [2020_07_07](presentation_2020_07_07)
  - Sparse Transformers
  - Publications:
    - Transformer - [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
    - Sparse Transformer - [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)
    - Image GPT (iGPT) - [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/)
    - Synthesizer - [Synthesizer: Rethinking Self-Attention in Transformer Models](https://arxiv.org/abs/2005.00743)
    - Longformer - [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)
    - Linformer - [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768)
    - Adaptive Attention Span - [Adaptive Attention Span in Transformers](https://arxiv.org/abs/1905.07799)
    - Adaptively Sparse Transformers - [Adaptively Sparse Transformers](https://arxiv.org/abs/1909.00015)
    - Reformer - [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
    - nBRC - [A bio-inspired bistable recurrent cell allows for long-lasting memory](https://arxiv.org/abs/2006.05252)
    - GShard - [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)
    - Sandwich Transformers - [Improving Transformer Models by Reordering their Sublayers](https://arxiv.org/abs/1911.03864)
    - MPNet - [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297)
- [2021_02_16](presentation_2021_02_16)
  - About distillation, a few words
  - Publications:
    - DistilBERT - [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)
    - TinyBERT - [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)
    - MobileBERT - [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984)
    - MiniLM - [MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers](https://arxiv.org/abs/2002.10957)
- [2021_02_25](presentation_2021_02_25)
  - LM again?
  - Publications:
    - Shortformer - [Shortformer: Better Language Modeling using Shorter Inputs](https://arxiv.org/abs/2012.15832)
    - Sparsely-Gated Mixture-of-Experts (Moe) - [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)
    - GShard - [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)
    - Switch Transformers - [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)
    - Feedback Transformers - [Addressing Some Limitations of Transformers with Feedback Memory](https://arxiv.org/abs/2002.09402)
    - GPT-Neo - [GPT Neo - code in GitHub](https://github.com/EleutherAI/gpt-neo)
- [2021_06_10](presentation_2021_06_10)
  - DeepSpeed ZeRO
  - Publications:
    - Mesh-Tensorflow - [Mesh-TensorFlow: Deep Learning for Supercomputers](https://arxiv.org/abs/1811.02084)
    - Megatron-LM - [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
    - Megatron-LM v2 - [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473)
    - L2L (layer-to-layer) - [Training Large Neural Networks with Constant Memory using a New Execution Algorithm](https://arxiv.org/abs/2002.05645)
    - PipeDream - [PipeDream: Generalized Pipeline Parallelism for DNN Training](https://www.microsoft.com/en-us/research/publication/pipedream-generalized-pipeline-parallelism-for-dnn-training/)
    - GPipe - [GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](https://arxiv.org/abs/1811.06965)
    - ZeRO - [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)
    - ZeRO-Offload - [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840)
    - ZeRO-Infinity - [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857)
- [2021_10_19](presentation_2021_10_19)
  - Zero-shot learning
  - Publications:
    - Prompting - [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)
    - Prefix-tuning - [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)
    - GPT-3 - [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
    - FLAN - [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)
- [2022_03_29](presentation_2022_03_29)
  - We need go ~~deeper~~ bigger
  - Publications:
    - Gopher - [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446)
    - GShard - [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)
    - ZeRO - [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)
    - ZeRO-Offload - [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840)
    - ZeRO-Infinity - [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857)
    - Megatron-LM - [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
    - Megatron-LM v2 - [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473)
    - Megatron-Turing NLG - [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/abs/2201.11990)
    - GPT-NeoX-20B - [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745)

---

# Models

- Adaptive Attention Span - [Adaptive Attention Span in Transformers](https://arxiv.org/abs/1905.07799)
- Adaptively Sparse Transformers - [Adaptively Sparse Transformers](https://arxiv.org/abs/1909.00015)
- ALBERT - [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
- BERT - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- CoVe - [Learned in Translation: Contextualized Word Vectors](https://arxiv.org/abs/1708.00107)
- CTRL - [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858)
- DeepNet - [DeepNet: Scaling Transformers to 1,000 Layers](https://arxiv.org/abs/2203.00555)
- DistilBERT - [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)
- ELMo - [Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples](https://arxiv.org/abs/1805.06556)
- Evolved Transformer - [The Evolved Transformer](https://arxiv.org/abs/1901.11117)
- Feedback Transformers - [Addressing Some Limitations of Transformers with Feedback Memory](https://arxiv.org/abs/2002.09402)
- FLAN - [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)
- Focused Hierarchical RNN - [Focused Hierarchical RNNs for Conditional Sequence Processing](https://arxiv.org/abs/1806.04342)
- Gopher - [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446)
- GPT-2 - [Language Models are Unsupervised Multitask Learners](https://openai.com/blog/better-language-models/)
- GPT-3 - [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- GPT-Neo - [GPT Neo - code in GitHub](https://github.com/EleutherAI/gpt-neo)
- GPT-NeoX-20B - [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745)
- GShard - [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)
- Image GPT (iGPT) - [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/)
- L2L (layer-to-layer) - [Training Large Neural Networks with Constant Memory using a New Execution Algorithm](https://arxiv.org/abs/2002.05645)
- LaMDA - [LaMDA: Language Models for Dialog Applications](https://arxiv.org/abs/2201.08239)
- Linformer - [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768)
- Longformer - [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)
- Megatron-LM - [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
- Megatron-LM v2 - [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473)
- Megatron-Turing NLG - [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/abs/2201.11990)
- MiniLM - [MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers](https://arxiv.org/abs/2002.10957)
- MobileBERT - [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984)
- MPNet - [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297)
- nBRC - [A bio-inspired bistable recurrent cell allows for long-lasting memory](https://arxiv.org/abs/2006.05252)
- PanGu - [PanGu: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation](https://arxiv.org/abs/2104.12369)
- PolyCoder - [A Systematic Evaluation of Large Language Models of Code](https://arxiv.org/abs/2202.13169)
- Prefix-tuning - [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)
- Prompting - [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)
- Reformer - [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
- RoBERTa - [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
- Sandwich Transformers - [Improving Transformer Models by Reordering their Sublayers](https://arxiv.org/abs/1911.03864)
- Shortformer - [Shortformer: Better Language Modeling using Shorter Inputs](https://arxiv.org/abs/2012.15832)
- Sparse Transformer - [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)
- Sparsely-Gated Mixture-of-Experts (Moe) - [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)
- StructBERT - [StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding](https://arxiv.org/abs/1908.04577)
- Switch Transformers - [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)
- Synthesizer - [Synthesizer: Rethinking Self-Attention in Transformer Models](https://arxiv.org/abs/2005.00743)
- T5 - [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
- TinyBERT - [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)
- Transformer - [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- TransformerXL - [Transformer-XL: Language Modeling with Longer-Term Dependency](https://openreview.net/forum?id=HJePno0cYm)
- XGLM - [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668)
- XLM - [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291)
- XLNet - [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)

---

# Benchmarks

- GLUE Benchmark - [GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://arxiv.org/abs/1804.07461)
- SuperGLUE Benchmark - [SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems](https://arxiv.org/abs/1905.00537)

---