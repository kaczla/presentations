@inproceedings{t64,
    author = {Al-Rfou, Rami and Choe, Dokook and Constant, Noah and Guo, Mandy and Jones, Llion},
    title = {Character-Level Language Modeling with Deeper Self-Attention},
    year = {2019},
    isbn = {978-1-57735-809-1},
    publisher = {AAAI Press},
    url = {https://doi.org/10.1609/aaai.v33i01.33013159},
    doi = {10.1609/aaai.v33i01.33013159},
    booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
    articleno = {388},
    numpages = {8},
    location = {Honolulu, Hawaii, USA},
    series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{charformer,
    title = {Charformer: Fast Character Transformers via Gradient-based Subword Tokenization},
    author = {Yi Tay and Vinh Q. Tran and Sebastian Ruder and Jai Gupta and Hyung Won Chung and Dara Bahri and Zhen Qin and Simon Baumgartner and Cong Yu and Donald Metzler},
    booktitle = {International Conference on Learning Representations},
    year = {2022},
    url = {https://openreview.net/forum?id=JtBRnrlOEFN}
}

@article{gpt2,
    title = {Language Models are Unsupervised Multitask Learners},
    author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    year = {2019}
}

@inproceedings{xlmr,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and
      Khandelwal, Kartikay  and
      Goyal, Naman  and
      Chaudhary, Vishrav  and
      Wenzek, Guillaume  and
      Guzm{\'a}n, Francisco  and
      Grave, Edouard  and
      Ott, Myle  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451"
}

@inproceedings{mt5,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498"
}

@article{byt5,
    title = "{B}y{T}5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models",
    author = "Xue, Linting  and
      Barua, Aditya  and
      Constant, Noah  and
      Al-Rfou, Rami  and
      Narang, Sharan  and
      Kale, Mihir  and
      Roberts, Adam  and
      Raffel, Colin",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.17",
    doi = "10.1162/tacl_a_00461",
    pages = "291--306"
}

@inproceedings{vtr_emb,
    title = "Robust Open-Vocabulary Translation from Visual Text Representations",
    author = "Salesky, Elizabeth  and
      Etter, David  and
      Post, Matt",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.576",
    doi = "10.18653/v1/2021.emnlp-main.576",
    pages = "7235--7252"
}

@article{nonparametric_mlm,
    title = {Nonparametric Masked Language Modeling},
    author = {Min, Sewon and Shi, Weijia and Lewis, Mike and Chen, Xilun and Yih, Wen-tau and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
    year = {2022}
}

@misc{paraformer,
    Author = {Xiaoyong Lu and Yaping Yan and Bin Kang and Songlin Du},
    Title = {ParaFormer: Parallel Attention Transformer for Efficient Feature Matching},
    Year = {2023},
    Eprint = {arXiv:2303.00941},
}

@misc{dropout,
    Author = {Zhuang Liu and Zhiqiu Xu and Joseph Jin and Zhiqiang Shen and Trevor Darrell},
    Title = {Dropout Reduces Underfitting},
    Year = {2023},
    Eprint = {arXiv:2303.01500},
}

@misc{hyena,
    Author = {Michael Poli and Stefano Massaroli and Eric Nguyen and Daniel Y. Fu and Tri Dao and Stephen Baccus and Yoshua Bengio and Stefano Ermon and Christopher RÃ©},
    Title = {Hyena Hierarchy: Towards Larger Convolutional Language Models},
    Year = {2023},
    Eprint = {arXiv:2302.10866},
}

@misc{spikegpt,
    Author = {Rui-Jie Zhu and Qihang Zhao and Jason K. Eshraghian},
    Title = {SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks},
    Year = {2023},
    Eprint = {arXiv:2302.13939},
}

@article{glm_130b,
    title = {GLM-130B: An Open Bilingual Pre-trained Model},
    author = {Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and Tam, Weng Lam and Ma, Zixuan and Xue, Yufei and Zhai, Jidong and Chen, Wenguang and Zhang, Peng and Dong, Yuxiao and Tang, Jie},
    journal = {arXiv preprint arXiv:2210.02414},
    year = {2022}
}

@misc{kosmos,
    Author = {Shaohan Huang and Li Dong and Wenhui Wang and Yaru Hao and Saksham Singhal and Shuming Ma and Tengchao Lv and Lei Cui and Owais Khan Mohammed and Barun Patra and Qiang Liu and Kriti Aggarwal and Zewen Chi and Johan Bjorck and Vishrav Chaudhary and Subhojit Som and Xia Song and Furu Wei},
    Title = {Language Is Not All You Need: Aligning Perception with Language Models},
    Year = {2023},
    Eprint = {arXiv:2302.14045},
}

@article{llama,
    title = {LLaMA: Open and Efficient Foundation Language Models},
    author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
    journal = {arXiv preprint arXiv:2302.13971},
    year = {2023}
}

@misc{lion,
    url = {https://arxiv.org/abs/2302.06675},
    author = {Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Liu, Yao and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and Le, Quoc V.},
    title = {Symbolic Discovery of Optimization Algorithms},
    publisher = {arXiv},
    year = {2023}
}

@inproceedings{palme,
    title = {PaLM-E: An Embodied Multimodal Language Model},
    author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
    booktitle = {arXiv preprint arXiv:2303.03378},
    year = {2023}
}

@software{rwkv,
    author = {PENG Bo},
    title = {BlinkDL/RWKV-LM: 0.01},
    month = aug,
    year = 2021,
    publisher = {Zenodo},
    version = {0.01},
    doi = {10.5281/zenodo.5196577},
    url = {https://doi.org/10.5281/zenodo.5196577}
}
